{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-069b67af28b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mEMBEDDING_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/virtualenvs/quoravenv/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1435\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/quoravenv/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb' '\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import utilities as u\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from gensim.models import KeyedVectors\n",
    "EMBEDDING_FILE = '../GoogleNews-vectors-negative300.bin.gz'\n",
    "model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../train.csv\")\n",
    "test_df = pd.read_csv(\"../test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Feature and Target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(train_df.columns[:-1])\n",
    "target_col = train_df.columns[-1]\n",
    "print (\"Feature Columns {}\".format(feature_cols))\n",
    "print (\"Target Columns {}\".format(target_col))\n",
    "X_all = train_df[feature_cols]\n",
    "y_all = pd.DataFrame(data=train_df[target_col], columns=[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collections.Counter(y_all['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_all.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_all.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check whether data is balanced or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.30, random_state=42)\n",
    "print (\"Total data set: {} samples\".format(X_all.shape[0]))\n",
    "print (\"Training set: {} samples\".format(X_train.shape[0]))\n",
    "print (\"Valid set: {} samples\".format(X_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collections.Counter(y_train['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collections.Counter(y_valid['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_valid.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_valid.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(words_list,df,column_name):\n",
    "    for sentence in df[column_name]:\n",
    "        for word in sentence:\n",
    "            words_list.append(word)\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_baseline_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_baseline_text('How do I play Pokémon GO in Korea?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_subset_df['question1'] = X_train['question1'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_subset_df_1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_subset_df_1['question1'] = X_train_subset_df['question1'].apply(lambda x:preprocess_baseline_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_subset_df_1['question1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all_1 = pd.DataFrame()\n",
    "test_df_1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_all_1['question1'] = X_all['question1'].apply(lambda x:preprocess_baseline_text(x))\n",
    "X_all_1['question2'] = X_all['question2'].apply(lambda x:preprocess_baseline_text(x))\n",
    "test_df_1['question1'] = test_df['question1'].apply(lambda x:preprocess_baseline_text(x))\n",
    "test_df_1['question2'] = test_df['question2'].apply(lambda x:preprocess_baseline_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_all_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_list = create_vocabulary([],X_all_1,'question1')\n",
    "print (\"Lenght of words in X_all question 1 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,X_all_1,'question2')\n",
    "print (\"Lenght of words after adding X_all question 2 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,test_df_1,'question1')\n",
    "print (\"Lenght of words after adding test_df question 1 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,test_df_1,'question2')\n",
    "print (\"Lenght of words after adding test_df question 2 {}\".format(len(words_list)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (len(set(words_list)))\n",
    "print (words_list[0:10])\n",
    "words_freq = collections.Counter(words_list)\n",
    "words_freq.most_common(10)\n",
    "words_freq_10000 = words_freq.most_common(10000)\n",
    "\n",
    "\n",
    "word_in_word2vec = []\n",
    "word_notin_word2vec = []\n",
    "\n",
    "for word in words_freq.most_common(10000):\n",
    "    if word[0] in model.vocab:\n",
    "        word_in_word2vec.append(word[0])\n",
    "    else:\n",
    "        word_notin_word2vec.append(word[0])\n",
    "        \n",
    "print (len(word_in_word2vec))\n",
    "print (len(word_notin_word2vec))\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (words_freq_10000[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (word_notin_word2vec[0:100])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (word_in_word2vec[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"[^A-Za-z0-9^?,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\?\", \" \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.reset_index(drop = True, inplace = True)\n",
    "X_valid.reset_index(drop = True, inplace = True)\n",
    "y_train.reset_index(drop = True, inplace = True)\n",
    "y_valid.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_train_df = pd.DataFrame()\n",
    "X_valid_df = pd.DataFrame()\n",
    "X_test_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_text('What is= the step by step guide to invest in?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_df['question1'] = X_train['question1'].apply(lambda x:preprocess_text(x))\n",
    "X_train_df['question2'] = X_train['question2'].apply(lambda x:preprocess_text(x))\n",
    "X_valid_df['question1'] = X_valid['question1'].apply(lambda x:preprocess_text(x))\n",
    "X_valid_df['question2'] = X_valid['question2'].apply(lambda x:preprocess_text(x))\n",
    "X_test_df['question1'] = test_df['question1'].apply(lambda x:preprocess_text(x))\n",
    "X_test_df['question2'] = test_df['question2'].apply(lambda x:preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283003,)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df['question1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of words in X_train_df question 1 3143028\n",
      "Lenght of words after adding X_train_df question 2 6358368\n",
      "Lenght of words after adding X_valid_df question 1 7705257\n",
      "Lenght of words after adding X_valid_df question 2 9080346\n",
      "Lenght of words after adding X_test_df question 1 48938305\n",
      "Lenght of words after adding X_test_df question 2 88928769\n"
     ]
    }
   ],
   "source": [
    "words_list = create_vocabulary([],X_train_df,'question1')\n",
    "print (\"Lenght of words in X_train_df question 1 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,X_train_df,'question2')\n",
    "print (\"Lenght of words after adding X_train_df question 2 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,X_valid_df,'question1')\n",
    "print (\"Lenght of words after adding X_valid_df question 1 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,X_valid_df,'question2')\n",
    "print (\"Lenght of words after adding X_valid_df question 2 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,X_test_df,'question1')\n",
    "print (\"Lenght of words after adding X_test_df question 1 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,X_test_df,'question2')\n",
    "print (\"Lenght of words after adding X_test_df question 2 {}\".format(len(words_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131743\n",
      "['how', 'is', 'the', 'working', 'environment', 'at', 'sbi', 'life', 'mumbai', 'how']\n",
      "9206\n",
      "794\n"
     ]
    }
   ],
   "source": [
    "print (len(set(words_list)))\n",
    "print (words_list[0:10])\n",
    "words_freq = collections.Counter(words_list)\n",
    "words_freq.most_common(10)\n",
    "words_freq_10000 = words_freq.most_common(10000)\n",
    "\n",
    "\n",
    "word_in_word2vec = []\n",
    "word_notin_word2vec = []\n",
    "\n",
    "for word in words_freq.most_common(10000):\n",
    "    if word[0] in model.vocab:\n",
    "        word_in_word2vec.append(word[0])\n",
    "    else:\n",
    "        word_notin_word2vec.append(word[0])\n",
    "        \n",
    "print (len(word_in_word2vec))\n",
    "print (len(word_notin_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'to', 'of', 'and', '-', 'quora', '2016', '10', 'instagram', '500', '1000', 'whatsapp', '2017', '2015', 'snapchat', '20', ':', '12', '100', '000', '15', '30', '50', 'jio', '12th', 'sbi', '16', '11', 'brexit', '!', '18', 'upsc', 'ece', '13', 'tcs', 'narendra', 'better:', '2014', '25', '17', '14', '70', 'mbbs', 'manipal', '2000', 'gmat', '40', 'iim', '24', 'btech', 'cgpa', '200', 'iiit', 'cgl', '10th', 'obc', 'redmi', 'favourite', '90', '60', 'iits', '21', 'pilani', 'aiims', 'centre', 'mightn', '80', 'flipkart', 'mustn', 'xiaomi', '19', 'travelling', 'ielts', '22', '300', 'india:', 'bba', 'colour', 'ibps', '23', 'ps4', '2013', 'mtech', 'accenture', 'x^2', 'paytm', '25000', 'elon', 'hadoop', 'kohli', 'srm', 'kejriwal', 'bitsat', 'spotify', '11th', 'grey', \"'\", '32', 'ncr', 'virat']\n"
     ]
    }
   ],
   "source": [
    "print (word_notin_word2vec[0:100]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_to_index = {}\n",
    "index_to_words = {}\n",
    "\n",
    "i = 0\n",
    "for word in set(words_list):\n",
    "    words_to_index[word] = i\n",
    "    i = i + 1\n",
    "    \n",
    "j=0\n",
    "for word in set(words_list):\n",
    "    index_to_words[j] = word\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('words_to_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(words_to_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('index_to_words.pickle', 'wb') as handle:\n",
    "    pickle.dump(index_to_words, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104295\n",
      "131743\n",
      "the\n",
      "offical\n"
     ]
    }
   ],
   "source": [
    "print (words_to_index['the'])\n",
    "print (len(words_to_index))\n",
    "print (index_to_words[104295])\n",
    "print (index_to_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19780, 69984, 104295, 12947, 38557, 71100]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_2_integer(wordslist):\n",
    "    question2integer = []\n",
    "    for word in wordslist:\n",
    "        question2integer.append(words_to_index[word])\n",
    "\n",
    "    return question2integer\n",
    "\n",
    "word_2_integer(['what', 'is', 'the', 'story', 'of', 'kohinoor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_df['question1'] = X_train_df['question1'].apply(lambda x:word_2_integer(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_df['question2'] = X_train_df['question2'].apply(lambda x:word_2_integer(x))\n",
    "X_valid_df['question1'] = X_valid_df['question1'].apply(lambda x:word_2_integer(x))\n",
    "X_valid_df['question2'] = X_valid_df['question2'].apply(lambda x:word_2_integer(x))\n",
    "X_test_df['question1'] = X_test_df['question1'].apply(lambda x:word_2_integer(x))\n",
    "X_test_df['question2'] = X_test_df['question2'].apply(lambda x:word_2_integer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [31586, 69984, 104295, 57112, 55384, 21628, 63...\n",
       "1    [31586, 126908, 23532, 84451, 69988, 115658, 9...\n",
       "2    [19780, 50711, 104295, 116401, 38557, 117493, ...\n",
       "3    [31586, 19743, 104295, 49658, 18288, 38557, 22...\n",
       "4    [69984, 104295, 1542, 29811, 72084, 47147, 416...\n",
       "Name: question1, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df['question1'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embed_length = 300\n",
    "embed_matrix = np.random.randn(len(words_to_index)+1,embed_length)\n",
    "# To ignore padding\n",
    "embed_matrix[0] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131744, 300)\n"
     ]
    }
   ],
   "source": [
    "print (embed_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.07708578e-01 -9.63583168e-01  1.24014304e+00  2.09291630e+00\n",
      "  7.82844117e-01  7.95844865e-01  1.83087362e+00  5.86118661e-01\n",
      "  3.23307614e-01 -3.93995365e-01  2.31290785e+00  5.35420298e-01\n",
      " -1.06687072e+00 -4.79405781e-01  1.94332778e+00  9.79287937e-01\n",
      "  4.90191438e-02 -1.78281724e-01 -2.49248430e+00  1.08787487e+00\n",
      " -2.00464048e+00 -1.48177593e+00 -4.66502725e-01 -4.89422453e-01\n",
      " -9.32083771e-01 -1.17234855e+00 -2.21491555e+00 -8.81211667e-01\n",
      "  4.75670694e-01  1.29135833e+00 -8.67405267e-02 -5.58238089e-01\n",
      " -6.41726175e-02  2.22534919e-01  1.22458135e+00  3.57438286e-01\n",
      " -1.78101599e+00  7.31911884e-01  3.42767622e-02 -5.21363852e-01\n",
      " -1.20148561e+00 -8.33140440e-01  4.30584783e-01  1.88149497e+00\n",
      " -8.27923703e-01 -1.46507591e-02  6.97847349e-01 -2.26018209e-01\n",
      "  7.88902921e-02 -5.32683041e-02  2.64329531e-01  1.38200321e+00\n",
      " -8.02914468e-01 -7.58184883e-01  2.37706989e-01 -1.96855939e+00\n",
      "  1.55745681e+00 -8.43194801e-01 -2.00822619e-01  4.85257283e-01\n",
      " -1.68236304e-01 -1.15431643e+00 -4.74760394e-01  4.08954347e-01\n",
      " -1.55097670e-01 -1.39054912e+00  1.43908652e-01 -1.87863435e+00\n",
      " -4.59754743e-03 -5.67159545e-01  3.27686583e-01 -4.50924969e-01\n",
      "  1.15249344e+00  9.53693947e-01  4.94376882e-01 -4.48666031e-01\n",
      " -4.94058575e-01  2.08304174e-01  4.79968051e-01  6.98962568e-01\n",
      " -2.64697390e-01  8.62655315e-01 -9.76214769e-01 -3.43279388e-01\n",
      " -4.20083346e-01 -3.57693856e-01  2.06452568e-01  3.39210221e-01\n",
      "  4.53717356e-01 -6.44733732e-01  1.30643353e+00  8.66602914e-02\n",
      " -4.97172854e-01  9.25158080e-01  1.18906283e-01 -1.26796269e-01\n",
      " -1.03214656e+00  1.27327339e+00 -8.46984779e-01 -6.11031846e-01\n",
      "  1.59879750e+00 -5.74021799e-01  1.02414071e+00 -6.99476581e-02\n",
      " -2.80240328e-01  2.17295865e-01 -2.74806898e-01 -1.62798702e+00\n",
      "  6.12682129e-01 -1.75990332e+00  3.88415653e-02 -3.92873282e-01\n",
      "  3.50603504e-01  4.81071261e-01  1.63692700e+00  1.76135756e+00\n",
      "  1.12761916e-01 -1.82297594e+00 -1.90948875e+00  7.21572252e-01\n",
      " -4.32818045e-01 -8.71728458e-01 -1.06091769e+00 -5.58197357e-01\n",
      " -2.25347563e+00 -9.04803750e-01 -2.38423641e+00 -5.64103675e-01\n",
      " -1.12918270e+00  5.30439928e-02  4.02960960e-01  6.07873918e-01\n",
      " -1.67935524e+00  9.69923102e-01 -6.62392597e-01 -1.45834348e+00\n",
      " -1.20228271e+00 -6.38520935e-01  2.23277037e-01  4.11035017e-01\n",
      "  2.96678036e-01  1.59507905e+00 -6.22357116e-01 -2.35584470e+00\n",
      "  8.93517331e-01  2.42668359e+00 -2.20435712e-02  1.52281147e+00\n",
      " -1.10572183e-01  5.23037931e-01  1.07836434e+00 -5.49642260e-01\n",
      " -4.60736098e-01 -1.54612966e-01  1.25237868e+00 -1.74930236e-01\n",
      " -1.06877281e+00  6.94436324e-01 -6.04260983e-01  9.74180039e-01\n",
      " -3.84287672e-01  2.01418383e+00  5.48110104e-01 -3.44855252e-01\n",
      " -2.83972243e+00  1.51449573e+00  8.49306615e-01 -5.65485412e-01\n",
      " -9.11372560e-01 -4.56125017e-01 -5.01352021e-01  1.27603159e+00\n",
      " -3.18735762e-01 -4.77570667e-01 -5.69910756e-02  9.55093874e-01\n",
      " -1.28591341e+00  3.52874608e-01 -8.43929452e-01  3.25626402e-01\n",
      " -5.68539268e-01 -2.21991491e+00 -4.30084514e-01  7.11899066e-01\n",
      "  4.35746001e-01 -9.88210644e-01 -7.15542055e-02 -6.32032464e-01\n",
      "  4.38812148e-01  1.16596911e+00  1.17908484e+00  1.29177068e+00\n",
      "  6.75840016e-02 -1.62418237e+00  5.64426097e-01  5.78075904e-01\n",
      " -1.69816997e+00  1.75558665e+00 -1.99165561e+00 -1.07690233e+00\n",
      "  4.69067180e-03 -8.83020237e-01 -5.34933732e-01  1.67782349e+00\n",
      "  1.32052470e+00 -7.92460660e-01 -4.18763807e-01 -1.21297290e+00\n",
      "  5.77546962e-01 -7.60742941e-01 -7.82004976e-01 -3.47019847e-01\n",
      " -9.39327198e-01 -1.34128350e-01 -3.61775290e-01  3.27242310e-01\n",
      " -2.98376689e-01 -1.27708015e+00 -4.32482088e-01 -8.81269727e-01\n",
      "  6.88173874e-01 -4.12062857e-01 -4.66699352e-02 -1.33105997e+00\n",
      "  5.39097929e-02 -9.82058417e-01 -5.70862493e-01 -4.36243166e-01\n",
      " -4.91056760e-01 -1.64113983e-01  7.85186124e-01 -5.59951535e-01\n",
      " -9.98177481e-01  1.57774373e-01  1.18459632e+00 -3.85271838e-01\n",
      " -3.73587471e-01  3.53537549e-01 -7.18307623e-01  2.56743981e-01\n",
      " -2.67913433e-01 -1.63367333e+00 -6.06786282e-04 -8.26981002e-01\n",
      " -4.96321140e-01  9.51094382e-01  5.91132231e-01 -1.11876797e+00\n",
      "  2.10167519e-01 -4.98825523e-01  1.16614924e+00 -1.70341236e-01\n",
      " -1.21874213e+00 -5.92010814e-03  1.64469093e+00 -8.63934101e-01\n",
      " -3.83277484e-01  6.90357746e-01 -1.21004251e+00  6.22122495e-01\n",
      "  8.98074832e-01 -1.65322874e-01  8.31063353e-01  1.46450577e+00\n",
      " -1.13995922e+00  6.02899570e-03 -2.62492751e-01 -1.68512775e+00\n",
      "  2.29822991e+00  1.76466891e-01  2.65274268e+00  1.34083429e+00\n",
      "  6.07518514e-01  1.19547274e+00 -1.94292619e-02 -2.42907904e-01\n",
      "  8.50711163e-01  1.17904085e+00 -5.18364073e-01  4.40400639e-02\n",
      "  6.46492660e-01 -1.32210073e+00 -2.89584240e-01 -6.11135890e-01\n",
      " -5.14413743e-01 -1.25490493e+00 -7.33001934e-01 -3.15255971e-01\n",
      "  1.47826976e+00  6.53359055e-01  2.15523533e-01  1.73500439e+00\n",
      "  1.69978077e+00  8.30094680e-01  1.39618562e+00 -4.40745350e-01\n",
      "  8.13406906e-01 -1.45127241e+00  2.72458294e-01 -5.74752298e-01]\n"
     ]
    }
   ],
   "source": [
    "print (embed_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58547\n"
     ]
    }
   ],
   "source": [
    "#Updating embedding matrix \n",
    "count = 0\n",
    "for word, index in words_to_index.items():\n",
    "    if word in model.vocab:\n",
    "        count = count + 1\n",
    "        embed_matrix[index] = model.word_vec(word)\n",
    "\n",
    "print (count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131743\n"
     ]
    }
   ],
   "source": [
    "print (len(words_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_seq_length = max([\n",
    "max(list(X_train_df['question1'].map(lambda x: len(x)))),\n",
    "max(list(X_train_df['question2'].map(lambda x: len(x)))),\n",
    "max(list(X_valid_df['question1'].map(lambda x: len(x)))),\n",
    "max(list(X_valid_df['question2'].map(lambda x: len(x)))),\n",
    "max(list(X_test_df['question1'].map(lambda x: len(x)))),\n",
    "max(list(X_test_df['question2'].map(lambda x: len(x)))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n"
     ]
    }
   ],
   "source": [
    "print (max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert labels to their numpy representations\n",
    "Y_train =  y_train.values\n",
    "Y_valid =  y_valid.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert X_train_dict['left'].shape == X_train_dict['right'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(X_train_dict['left']) == len(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#padding to max seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31586, 69984, 104295, 57112, 55384, 21628, 63296, 131479, 95639]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df['question1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_left_zeros(question_list,max_seq_length):\n",
    "    question_list = [0] * (max_seq_length - len(question_list)) + question_list\n",
    "    return question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (pad_left_zeros([31586, 69984, 104295, 57112, 55384, 21628, 63296, 131479, 95639]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_df['question1'] = X_train_df['question1'].apply(lambda x: pad_left_zeros(x,max_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_df['question2'] = X_train_df['question2'].apply(lambda x: pad_left_zeros(x,max_seq_length))\n",
    "X_valid_df['question1'] = X_valid_df['question1'].apply(lambda x: pad_left_zeros(x,max_seq_length))\n",
    "X_valid_df['question2'] = X_valid_df['question2'].apply(lambda x: pad_left_zeros(x,max_seq_length))\n",
    "X_test_df['question1'] = X_test_df['question1'].apply(lambda x: pad_left_zeros(x,max_seq_length))\n",
    "X_test_df['question2'] = X_test_df['question2'].apply(lambda x: pad_left_zeros(x,max_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split to dicts\n",
    "X_train_dict = {'left': X_train_df['question1'], 'right': X_train_df['question2']}\n",
    "X_valid_dict = {'left': X_valid_df['question1'], 'right': X_valid_df['question2']}\n",
    "X_test_dict = {'left': X_test_df['question1'], 'right': X_test_df['question2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dim = 300\n",
    "timesteps = 242\n",
    "nb_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "#Input None,max_seq_length,1\n",
    "\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(embed_matrix),\n",
    "                            embed_length,\n",
    "                            weights=[embed_matrix],\n",
    "                            input_length=max_seq_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(242), Dimension(300)])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_left.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This layer can take as input a matrix\n",
    "# and will return a vector of size 64\n",
    "shared_lstm = LSTM(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(64)])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can then concatenate the two vectors:\n",
    "merged_vector = keras.layers.concatenate([left_output,right_output], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And add a logistic regression on top\n",
    "predictions = Dense(1, activation='sigmoid')(merged_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_dict['left'] = np.concatenate(X_train_dict['left']).reshape(X_train_dict['left'].shape[0],max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283003, 242)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dict['left'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_dict['right'] = np.concatenate(X_train_dict['right']).reshape(X_train_dict['right'].shape[0],max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283003, 242)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dict['right'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid_dict['left'] = np.concatenate(X_valid_dict['left']).reshape(X_valid_dict['left'].shape[0],max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid_dict['right'] = np.concatenate(X_valid_dict['right']).reshape(X_valid_dict['right'].shape[0],max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,  31586, 126886,  69984, 115658,  38557,\n",
       "        63296,  42336])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dict['right'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 283003 samples, validate on 121287 samples\n",
      "Epoch 1/1\n",
      "283003/283003 [==============================] - 1392s 5ms/step - loss: 0.5472 - acc: 0.7226 - val_loss: 0.5324 - val_acc: 0.7328\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-340-146250399540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m          validation_data=([X_valid_dict['left'], X_valid_dict['right']], Y_valid))\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training time finished.\\n{} epochs in {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtraining_start_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'n_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# We define a trainable model linking the\n",
    "# tweet inputs to the predictions\n",
    "model = Model(inputs=[left_input, right_input], outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "n_epoch = 1\n",
    "# Start training\n",
    "training_start_time = time()\n",
    "\n",
    "model.fit([X_train_dict['left'], X_train_dict['right']], Y_train, batch_size=128, epochs=n_epoch,\n",
    "         validation_data=([X_valid_dict['left'], X_valid_dict['right']], Y_valid))\n",
    "\n",
    "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 283003 samples, validate on 121287 samples\n",
      "Epoch 1/4\n",
      "283003/283003 [==============================] - 1382s 5ms/step - loss: 0.5136 - acc: 0.7459 - val_loss: 0.5168 - val_acc: 0.7473\n",
      "Epoch 2/4\n",
      "283003/283003 [==============================] - 1427s 5ms/step - loss: 0.4901 - acc: 0.7606 - val_loss: 0.5034 - val_acc: 0.7538\n",
      "Epoch 3/4\n",
      "283003/283003 [==============================] - 1352s 5ms/step - loss: 0.4709 - acc: 0.7731 - val_loss: 0.4979 - val_acc: 0.7608\n",
      "Epoch 4/4\n",
      "283003/283003 [==============================] - 1461s 5ms/step - loss: 0.4537 - acc: 0.7827 - val_loss: 0.4950 - val_acc: 0.7655\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-75f53b2a4683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m          validation_data=([X_valid_dict['left'], X_valid_dict['right']], Y_valid))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training time finished.\\n{} epochs in {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtraining_start_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "n_epoch = 4\n",
    "training_start_time = time()\n",
    "\n",
    "model.fit([X_train_dict['left'], X_train_dict['right']], Y_train, batch_size=128, epochs=n_epoch,\n",
    "         validation_data=([X_valid_dict['left'], X_valid_dict['right']], Y_valid))\n",
    "\n",
    "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 283003 samples, validate on 121287 samples\n",
      "Epoch 1/20\n",
      "283003/283003 [==============================] - 1464s 5ms/step - loss: 0.4383 - acc: 0.7910 - val_loss: 0.4934 - val_acc: 0.7683\n",
      "Epoch 2/20\n",
      "283003/283003 [==============================] - 1463s 5ms/step - loss: 0.4236 - acc: 0.8003 - val_loss: 0.5015 - val_acc: 0.7572\n",
      "Epoch 3/20\n",
      "283003/283003 [==============================] - 1327s 5ms/step - loss: 0.4104 - acc: 0.8071 - val_loss: 0.5007 - val_acc: 0.7629\n",
      "Epoch 4/20\n",
      "283003/283003 [==============================] - 1316s 5ms/step - loss: 0.3980 - acc: 0.8145 - val_loss: 0.5047 - val_acc: 0.7695\n",
      "Epoch 5/20\n",
      "283003/283003 [==============================] - 1314s 5ms/step - loss: 0.3868 - acc: 0.8212 - val_loss: 0.5091 - val_acc: 0.7675\n",
      "Epoch 6/20\n",
      "283003/283003 [==============================] - 1318s 5ms/step - loss: 0.3762 - acc: 0.8259 - val_loss: 0.5151 - val_acc: 0.7649\n",
      "Epoch 7/20\n",
      "283003/283003 [==============================] - 1317s 5ms/step - loss: 0.3663 - acc: 0.8321 - val_loss: 0.5228 - val_acc: 0.7728\n",
      "Epoch 8/20\n",
      "283003/283003 [==============================] - 1316s 5ms/step - loss: 0.3570 - acc: 0.8372 - val_loss: 0.5279 - val_acc: 0.7689\n",
      "Epoch 9/20\n",
      "283003/283003 [==============================] - 1316s 5ms/step - loss: 0.3477 - acc: 0.8421 - val_loss: 0.5343 - val_acc: 0.7704\n",
      "Epoch 10/20\n",
      "283003/283003 [==============================] - 1318s 5ms/step - loss: 0.3393 - acc: 0.8462 - val_loss: 0.5478 - val_acc: 0.7709\n",
      "Epoch 11/20\n",
      "283003/283003 [==============================] - 1330s 5ms/step - loss: 0.3312 - acc: 0.8513 - val_loss: 0.5536 - val_acc: 0.7706\n",
      "Epoch 12/20\n",
      "283003/283003 [==============================] - 1311s 5ms/step - loss: 0.3236 - acc: 0.8541 - val_loss: 0.5569 - val_acc: 0.7702\n",
      "Epoch 13/20\n",
      "283003/283003 [==============================] - 1314s 5ms/step - loss: 0.3158 - acc: 0.8589 - val_loss: 0.5730 - val_acc: 0.7711\n",
      "Epoch 14/20\n",
      "283003/283003 [==============================] - 1314s 5ms/step - loss: 0.3088 - acc: 0.8630 - val_loss: 0.5776 - val_acc: 0.7650\n",
      "Epoch 15/20\n",
      "283003/283003 [==============================] - 1312s 5ms/step - loss: 0.3018 - acc: 0.8662 - val_loss: 0.5885 - val_acc: 0.7586\n",
      "Epoch 16/20\n",
      "283003/283003 [==============================] - 1312s 5ms/step - loss: 0.2952 - acc: 0.8696 - val_loss: 0.5892 - val_acc: 0.7668\n",
      "Epoch 17/20\n",
      "283003/283003 [==============================] - 1313s 5ms/step - loss: 0.2888 - acc: 0.8726 - val_loss: 0.6065 - val_acc: 0.7664\n",
      "Epoch 18/20\n",
      "283003/283003 [==============================] - 1313s 5ms/step - loss: 0.2828 - acc: 0.8756 - val_loss: 0.6171 - val_acc: 0.7648\n",
      "Epoch 19/20\n",
      "283003/283003 [==============================] - 1313s 5ms/step - loss: 0.2773 - acc: 0.8782 - val_loss: 0.6267 - val_acc: 0.7636\n",
      "Epoch 20/20\n",
      "283003/283003 [==============================] - 1314s 5ms/step - loss: 0.2713 - acc: 0.8809 - val_loss: 0.6303 - val_acc: 0.7657\n",
      "Training time finished.\n",
      "20 epochs in 7:23:34.362409\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "training_start_time = time()\n",
    "\n",
    "model.fit([X_train_dict['left'], X_train_dict['right']], Y_train, batch_size=128, epochs=n_epoch,\n",
    "         validation_data=([X_valid_dict['left'], X_valid_dict['right']], Y_valid))\n",
    "\n",
    "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "# # save the tokenizer and model\n",
    "# with open(\"keras_tokenizer.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(tokenizer, f)\n",
    "model.save(\"quora_keras_model_v1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('words_to_index.pickle', 'rb') as handle:\n",
    "    words_to_index_1 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('index_to_words.pickle', 'rb') as handle:\n",
    "    index_to_words_1 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131743\n"
     ]
    }
   ],
   "source": [
    "print (len(words_to_index_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131743\n"
     ]
    }
   ],
   "source": [
    "print (len(index_to_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offical\n"
     ]
    }
   ],
   "source": [
    "print (index_to_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newtexts = [\"How do I read and find my YouTube comments?\", \"How can I see all my Youtube comments?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How do I read and find my YouTube comments?', 'How can I see all my Youtube comments?']\n"
     ]
    }
   ],
   "source": [
    "print (newtexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19780, 58041, 96722, 19743, 95526, 83139, 23532, 29267, 92085]\n",
      "How do I read and find my YouTube comments?\n",
      "how\n",
      "do\n",
      "i\n",
      "read\n",
      "and\n",
      "find\n",
      "my\n",
      "youtube\n",
      "comments\n",
      "How can I see all my Youtube comments?\n",
      "how\n",
      "can\n",
      "i\n",
      "see\n",
      "all\n",
      "my\n",
      "youtube\n",
      "comments\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_new_words_list = []\n",
    "print (new_words_list)\n",
    "i = 0\n",
    "for words_list in newtexts:\n",
    "    new_words_list = []\n",
    "    print (words_list)\n",
    "    for word in preprocess_text(words_list):\n",
    "        print (word)\n",
    "        if words_to_index_1.get(word):\n",
    "            \n",
    "            new_words_list.append(words_to_index_1.get(word))\n",
    "        else:\n",
    "            print (word)\n",
    "    final_new_words_list.append(np.array(pad_left_zeros(new_words_list,max_seq_length)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([     0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,  31586,  19743,  96722, 128849,  55922,  73643,  95083,\n",
      "        76906,  12805]), array([     0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,  31586, 126908,  96722, 105700,  49832,  95083,\n",
      "        76906,  12805])]\n"
     ]
    }
   ],
   "source": [
    "print (final_new_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "predict_model = load_model(\"quora_keras_model_v1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 1s 751ms/step\n"
     ]
    }
   ],
   "source": [
    "y_prob = predict_model.predict([final_new_words_list[0].reshape(1,max_seq_length), final_new_words_list[1].reshape(1,max_seq_length)],batch_size=1, verbose=1, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_classes = y_prob.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print (y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01316933]]\n"
     ]
    }
   ],
   "source": [
    "print (y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1rnn = Sequential()\n",
    "s1rnn.add(embedding_layer_1)\n",
    "s1rnn.add(LSTM(128, input_shape=(100, 1)))\n",
    "s1rnn.add(Dense(1))\n",
    "\n",
    "s2rnn = Sequential()\n",
    "s2rnn.add(embedding_layer_2)\n",
    "s2rnn.add(LSTM(128, input_shape=(100, 1)))\n",
    "s2rnn.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_baseline_text(X_train['question1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# re.sub(r\"[^A-Za-z0-9,!.\\/'+-=]\", \" \", 'why am i mentally very lonely? how can i solve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# re.sub(r\"\\'s\", \" \", 'what\\'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words_list = []\n",
    "# # train_subset_df = train_df['question1'][0:10]\n",
    "# train_subset_df['question1'] = pd.DataFrame(data=train_df['question1'][0:10], columns=['question1'])\n",
    "# train_subset_df['question2'] = pd.DataFrame(data=train_df['question2'][0:10], columns=['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for sentence in train_subset_df['question1']:\n",
    "#     for word in sentence:\n",
    "#         words_list.append(word)\n",
    "# print (len(set(words_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature based on how many words are common in question 1 and question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# u.termfrequency(['What is the step by step guide to invest in share market in india?'], ['What is the step by step guide to invest in share market?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def termfrequency(sentence1, sentence2):\n",
    "   \n",
    "    question_dict ={}\n",
    "    sentence1_words = sentence1   \n",
    "    sentence2_words = sentence2\n",
    "    searchtermfreq = []\n",
    "    i = 0\n",
    "    \n",
    "    for key in sentence1_words:\n",
    "#         print (key)\n",
    "        question_dict[key] = question_dict.get(key,0) + 1\n",
    "    \n",
    "    for key in set(sentence2_words):\n",
    "        value =  question_dict.get(key,0)\n",
    "        if value >= 1:\n",
    "            value = 1\n",
    "        searchtermfreq.append(value)\n",
    "        \n",
    "    \n",
    "#     print (question_dict)\n",
    "#     print (searchtermfreq)\n",
    "#     print (sum(searchtermfreq))\n",
    "    return sum(searchtermfreq)\n",
    "\n",
    "termfrequency(['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india?'], ['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market?'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_df['common_term_freq'] = X_train_df.apply(lambda x: termfrequency(x['question1'],x['question2']), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid_df['common_term_freq'] = X_valid_df.apply(lambda x: termfrequency(x['question1'],x['question2']), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_valid_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total words frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_words_freq(sentence):\n",
    "    return len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words_freq(['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_df['question1_words_freq'] = X_train_df['question1'].map(lambda x: total_words_freq(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_df['question2_words_freq'] = X_train_df['question2'].map(lambda x: total_words_freq(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid_df['question1_words_freq'] = X_valid_df['question1'].map(lambda x: total_words_freq(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid_df['question2_words_freq'] = X_valid_df['question2'].map(lambda x: total_words_freq(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_valid_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_model_input = X_train_df.drop(['question1','question2'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid_model_input = X_valid_df.drop(['question1','question2'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_model_input.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train a model# Train  \n",
    "import time\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    print (\"Training {}...\".format(clf.__class__.__name__))\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print (\"Done!\\nTraining time (secs): {:.3f}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf =  LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_classifier(clf, X_train_model_input, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def predict_labels(clf, X_train, y_train):\n",
    "    print (\"Predicting labels using {}...\".format(clf.__class__.__name__))\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(X_train)\n",
    "    end = time.time()\n",
    "    print (\"Done!\\nPrediction time (secs): {:.3f}\".format(end - start))\n",
    "    return log_loss(y_train, y_pred, eps=1e-15), confusion_matrix(y_train, y_pred)\n",
    "\n",
    "train_metrics = predict_labels(clf, X_train_model_input, y_train.values.ravel())\n",
    "\n",
    "print \n",
    "print (\"Log loss for training set: {}\".format(train_metrics[0]))\n",
    "\n",
    "print (\"Confusion matrix for training set: {}\".format(train_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "print (\"Log loss for validation set: {}\".format(predict_labels(clf, X_valid_model_input, y_valid.values.ravel())[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print (\"Confusion matrix for validation set: {}\".format(predict_labels(clf, X_valid_model_input, y_valid.values.ravel())[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_list = create_vocabulary([],X_train_df,'question1')\n",
    "print (\"Lenght of words in X_train_df question 1 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,X_train_df,'question2')\n",
    "print (\"Lenght of words after adding X_train_df question 2 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,X_valid_df,'question1')\n",
    "print (\"Lenght of words after adding X_valid_df question 1 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,X_valid_df,'question2')\n",
    "print (\"Lenght of words after adding X_valid_df question 2 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,X_test_df,'question1')\n",
    "print (\"Lenght of words after adding X_test_df question 1 {}\".format(len(words_list)))\n",
    "words_list = create_vocabulary(words_list,X_test_df,'question2')\n",
    "print (\"Lenght of words after adding X_test_df question 2 {}\".format(len(words_list)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(list_words):\n",
    "     list_words_processed = []\n",
    "     for text in list_words:\n",
    "         text = re.sub(r\"\\?\", '', text)\n",
    "         text = re.sub(r\"i'm\", \"i am \", text)\n",
    "#          print (text)\n",
    "         list_words_processed.append(str(text))\n",
    "#          print (list_words_processed)\n",
    "     return list_words_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_text(['India?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_text(['how',\n",
    " 'do',\n",
    " 'the',\n",
    " 'holy',\n",
    " 'scriptures',\n",
    " 'of',\n",
    " 'hinduism',\n",
    " 'compare',\n",
    " 'and',\n",
    " 'contrast',\n",
    " 'to',\n",
    " 'those',\n",
    " 'of',\n",
    " 'taoism?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_df['question1'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_text(X_train_df['question1'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_p_df = pd.DataFrame()\n",
    "X_valid_p_df = pd.DataFrame()\n",
    "X_test_p_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_p_df['question1'] = X_train_df['question1'].apply(lambda x:preprocess_text(x))\n",
    "X_train_p_df['question2'] = X_train_df['question2'].apply(lambda x:preprocess_text(x))\n",
    "X_valid_p_df['question1'] = X_valid_df['question1'].apply(lambda x:preprocess_text(x))\n",
    "X_valid_p_df['question2'] = X_valid_df['question2'].apply(lambda x:preprocess_text(x))\n",
    "X_test_p_df['question1'] = X_test_df['question1'].apply(lambda x:preprocess_text(x))\n",
    "X_test_p_df['question2'] = X_test_df['question2'].apply(lambda x:preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_p_df['question1'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proc_words_list = create_vocabulary([],X_train_p_df,'question1')\n",
    "print (\"Lenght of words in X_train_df question 1 {}\".format(len(words_list)))\n",
    "proc_words_list = create_vocabulary(words_list,X_train_p_df,'question2')\n",
    "print (\"Lenght of words after adding X_train_df question 2 {}\".format(len(words_list)))\n",
    "proc_words_list = create_vocabulary(words_list,X_valid_p_df,'question1')\n",
    "print (\"Lenght of words after adding X_valid_df question 1 {}\".format(len(words_list)))\n",
    "proc_words_list = create_vocabulary(words_list,X_valid_p_df,'question2')\n",
    "print (\"Lenght of words after adding X_valid_df question 2 {}\".format(len(words_list)))\n",
    "proc_words_list = create_vocabulary(words_list,X_test_p_df,'question1')\n",
    "print (\"Lenght of words after adding X_test_df question 1 {}\".format(len(words_list)))\n",
    "proc_words_list = create_vocabulary(words_list,X_test_p_df,'question2')\n",
    "print (\"Lenght of words after adding X_test_df question 2 {}\".format(len(words_list)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proc_words_freq = collections.Counter(proc_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proc_words_freq_10000 = proc_words_freq.most_common(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proc_word_in_word2vec = []\n",
    "proc_word_notin_word2vec = []\n",
    "\n",
    "for word in proc_words_freq.most_common(10000):\n",
    "    if word[0] in model.vocab:\n",
    "        proc_word_in_word2vec.append(word[0])\n",
    "    else:\n",
    "        proc_word_notin_word2vec.append(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (len(proc_word_in_word2vec))\n",
    "print (len(proc_word_notin_word2vec))\n",
    "print (proc_word_notin_word2vec[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (proc_word_in_word2vec[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "quoravenv",
   "language": "python",
   "name": "quoravenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
